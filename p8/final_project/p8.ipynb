{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify Fraud from Enron Email\n",
    "\n",
    "Jeremy Tran, June 13, 2018\n",
    "\n",
    "\n",
    "\n",
    "### Dataset and Questions\n",
    "\n",
    "Machine learning uses computing and statistics to generate findings, which are only as good as the data used.  The Enron scandal was this huge messy affair, and unsurprisingly the data associated with it is also huge and messy, which makes it a suitable application for machine learning to see if certain questions can be answered.  Namely, in this project, the question is if persons of interest (POIs) can be identified from the financial and e-mail data.\n",
    "\n",
    "The dataset as initially prepared contains information for 146 individuals, with up to 21 features for finances, e-mail use and status as a POI.  There are 18 POIs total to be identified.\n",
    "\n",
    "As explained in the course material, one blatant outlier is the `TOTAL` \"individual,\" so I removed that entry first.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no of persons:  145\n",
      "no of POIs:  18\n",
      "no of NaN values:  1352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jt002\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data, test_classifier\n",
    "\n",
    "with open(\"final_project_dataset.pkl\", \"rb\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "\n",
    "my_dataset = data_dict\n",
    "my_dataset.pop('TOTAL')\n",
    "\n",
    "### Printing key characteristics of dataset\n",
    "print('no of persons: ', len(data_dict))\n",
    "print('no of POIs: ',\n",
    "      len(['poi' for poi in data_dict if data_dict[poi]['poi'] == 1]))\n",
    "print('no of NaN values: ',\n",
    "      len([(person,feature)\n",
    "           for person in data_dict\n",
    "           for feature in data_dict[person]\n",
    "           if data_dict[person][feature] == 'NaN']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "\n",
    "_**New Feature**_\n",
    "\n",
    "As per assignment requirements, I had to create a new feature, and seeing as I'm not an expert in mining tons of text data, I opted simply... to create a new feature that would be the agglomeration of all financial info for each individual, and see if, after being appropriately scaled and undergoing pca, etc, it could replace the disparate myriad finanical details as features and, in conjunction, with email features, classify POIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Worth\" as a Feature\n",
      "Naive Bayes Accuracy:  0.84 \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    non-POI       0.88      0.94      0.91        88\n",
      "        POI       0.17      0.08      0.11        12\n",
      "\n",
      "avg / total       0.80      0.84      0.82       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Adding new feature... creative I know\n",
    "finance = ['salary','deferral_payments','total_payments','loan_advances',\n",
    "           'bonus','restricted_stock_deferred','deferred_income','total_stock_value',\n",
    "           'expenses','exercised_stock_options','other','long_term_incentive',\n",
    "           'restricted_stock','director_fees']\n",
    "\n",
    "for person in my_dataset:\n",
    "    my_dataset[person]['worth'] = sum(\n",
    "        my_dataset[person][f]\n",
    "        for f in finance\n",
    "        if my_dataset[person][f] != 'NaN')\n",
    "\n",
    "### Testing replacing all financial features with 'worth'\n",
    "features_list = ['poi', 'worth', 'shared_receipt_with_poi', 'from_poi_to_this_person',\n",
    "               'from_this_person_to_poi']\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "labels, features = targetFeatureSplit(data)\n",
    "features_test, features_train, labels_test, labels_train = train_test_split(\n",
    "    features, labels, test_size = 0.3, random_state = 42)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "target_names = ['non-POI','POI']\n",
    "nb = GaussianNB()\n",
    "nb.fit(features_train, labels_train)\n",
    "\n",
    "print('\"Worth\" as a Feature')\n",
    "print('Naive Bayes Accuracy: ', nb.score(features_test, labels_test), '\\n')\n",
    "print(classification_report(labels_test, nb.predict(features_test),\n",
    "                            target_names = target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "My preliminary testing showed that this aggregate variable did not vastly improve predictions when added alongside existing financial features to classifiers.  This was perhaps expected, as it is simply a duplicate of the existing information and perhaps augments certain selected financial features.  However, when used in lieu of all financial features, and coupled with only e-mail information, this combined 'worth' value did not fare well in predicting any POIs at all...\n",
    "\n",
    "That certain financial features were more predictive than net worth alone reflected that simple wealth or large amounts of assets were not peculiar to POIs... though they did tend to have higher net asset values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Univariate feature selection**_\n",
    "\n",
    "I decided to use a univariate feature selection method, SelectKBest, since I'm not very creative.  In this method, an ANOVA f-score is calculated for each feature against the classification labels, and thus the importance of each feature can be guesstimated.  In a way, I felt like this was cheating since I was essentially using the entire dataset to find the important features, and then later making predictions using subsets of that dataset based on those features... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features and ANOVA Scores\n",
      "\n",
      "exercised_stock_options :  25.09754152873549\n",
      "total_stock_value :  24.4676540475264\n",
      "bonus :  21.06000170753657\n",
      "salary :  18.575703268041785\n",
      "deferred_income :  11.5955476597306\n",
      "long_term_incentive :  10.072454529369441\n",
      "restricted_stock :  9.346700791051488\n",
      "total_payments :  8.866721537107772\n",
      "shared_receipt_with_poi :  8.74648553212908\n",
      "loan_advances :  7.242730396536018\n",
      "expenses :  6.23420114050674\n",
      "from_poi_to_this_person :  5.344941523147337\n",
      "other :  4.204970858301416\n",
      "from_this_person_to_poi :  2.426508127242878\n",
      "director_fees :  2.107655943276091\n",
      "to_messages :  1.69882434858085\n",
      "deferral_payments :  0.2170589303395084\n",
      "from_messages :  0.16416449823428736\n",
      "restricted_stock_deferred :  0.06498431172371151\n"
     ]
    }
   ],
   "source": [
    "### Preparing data for feature selection\n",
    "all_features_list = ['poi']\n",
    "nonpoi_features = list(my_dataset['LAY KENNETH L'])\n",
    "nonpoi_features.remove('email_address')\n",
    "nonpoi_features.remove('poi')\n",
    "nonpoi_features.remove('worth')\n",
    "all_features_list.extend(nonpoi_features)\n",
    "all_data = featureFormat(my_dataset, all_features_list, sort_keys = True)\n",
    "all_labels, all_features = targetFeatureSplit(all_data)\n",
    "\n",
    "### Using SelectKBest to determine features importances\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "kbest = SelectKBest(k = 'all')\n",
    "kbest.fit(all_features, all_labels)\n",
    "best_order = np.argsort(kbest.scores_)[::-1]\n",
    "\n",
    "print(\"Features and ANOVA Scores\\n\")\n",
    "for i in range(len(best_order)):\n",
    "    print(nonpoi_features[best_order[i]], \": \", kbest.scores_[best_order[i]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From examining the scores, I found that certain financial features showed huge variances between POIs and non-POIs, with the rest of the features, including both financial and e-mail information, being less important.\n",
    "\n",
    "I decided to use `GridSearchCV` to test all the possible number of k best features as run through a classifier (randomly chosen to be Naive Bayes), using f1 as the performance score.\n",
    "\n",
    "(idea for this code courtesy of user4646875 https://stackoverflow.com/questions/44999289/print-feature-names-for-selectkbest-where-k-value-is-inside-param-grid-of-gridse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best f1 score:  0.39580808080808083\n",
      "best k parameter: {'kbest__k': 5}\n",
      "best k=5 features:  ['salary', 'bonus', 'deferred_income', 'total_stock_value', 'exercised_stock_options']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "nb = GaussianNB()\n",
    "pipeline = Pipeline(steps = [('kbest', kbest), ('nb', nb)])\n",
    "\n",
    "k_options = list(range(len(all_features_list)))[1:]\n",
    "param_grid = {'kbest__k': k_options}\n",
    "cv = StratifiedShuffleSplit(test_size = 0.3, random_state = 70)\n",
    "\n",
    "gridsearch = GridSearchCV(pipeline, param_grid = param_grid, scoring = 'f1', cv = cv)\n",
    "gridsearch.fit(all_features, all_labels)\n",
    "\n",
    "### Incorporating k best features found via gridsearch\n",
    "best_features = [nonpoi_features[i] for i in\n",
    "                 gridsearch.best_estimator_.named_steps['kbest'].get_support(indices=True)]\n",
    "features_list = ['poi']\n",
    "features_list.extend(best_features)\n",
    "\n",
    "### Printing best parameters of select k best\n",
    "print('best f1 score: ', gridsearch.best_score_)\n",
    "print('best k parameter:', gridsearch.best_params_)\n",
    "print('best k=5 features: ', best_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And... it turned out the best estimator came from using only the 5 best features, whose anova scores were also all above 10 in the above examination.\n",
    "\n",
    "_**Feature Scaling**_\n",
    "\n",
    "Since I was going to be testing a bunch of algorithms, some of which may depend on scaling of feature values (e.g. SVM), I decided to apply MinMaxScaler to all of the features.  Since scaling technically modified my dataset, I probably should've concatenated the features and labels after scaling, saved them as `my_dataset`, and _then_ split them again into labels and features for the subsequent steps...or maybe not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Extracting features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "### Scaling features for certain algorithms\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "features = scaler.fit_transform(features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Picking and Tuning Algorithims\n",
    "\n",
    "_**Prelimnary**_\n",
    "\n",
    "For picking algorithms initially, using the built-in in score function of each classifier to test accuracy was probably a sufficient performance measure.  But since we are also concerned with precision and recall in this project (and by extension, the f1 score as the averaged mean of precision and recall), I found a function in `sklearn.metrics` that computes all of the above and thus imported it for use for evaluating algorithms.\n",
    "\n",
    "Actually, to begin training the algorithms I needed to use a cross-validation method, which is tied to training and testing.  Since only 18 out of 144 labels are POIs, a stratified splitting strategy made sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generating testing and training data\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "sss = StratifiedShuffleSplit(test_size = 0.3, random_state = 47)\n",
    "\n",
    "features_train = []\n",
    "features_test  = []\n",
    "labels_train   = []\n",
    "labels_test    = []\n",
    "for train_idx, test_idx in sss.split(features, labels):\n",
    "    for ii in train_idx:\n",
    "        features_train.append(features[ii])\n",
    "        labels_train.append(labels[ii])\n",
    "    for jj in test_idx:\n",
    "        features_test.append(features[jj])\n",
    "        labels_test.append(labels[jj])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Naive Bayes**_\n",
    "\n",
    "Not much to play around with here, and performance seemed reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy:  0.9 \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    non-POI       0.93      0.96      0.94       370\n",
      "        POI       0.61      0.44      0.51        50\n",
      "\n",
      "avg / total       0.89      0.90      0.89       420\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nbc = GaussianNB()\n",
    "nbc.fit(features_train, labels_train)\n",
    "\n",
    "print('Naive Bayes Accuracy: ', nbc.score(features_test, labels_test), '\\n')\n",
    "print(classification_report(labels_test, nbc.predict(features_test),\n",
    "                            target_names = target_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "_**Tuning SVM Parameters with GridSearchCV**_\n",
    "\n",
    "Tuning algorithm parameters is important as typically there is a tradeoff between variance and bias.  High bias algorithms may fail to predict the underlying pattern, while high variance models may be responding more to noise than actual relations in the data.\n",
    "\n",
    "I used GridSearchCV to test various parameters with SVM, and the best-performing ones seemed... highly unorthrodox, I guess?  Default values for C and gamma are 1 and 1/(sample size), respectively.  The accuracy score also seemed highly suspect, possibly reflecting overfitting..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:  {'C': 100, 'gamma': 10.0}\n",
      "SVM Accuracy:  0.9666666666666667 \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    non-POI       0.96      1.00      0.98       370\n",
      "        POI       1.00      0.72      0.84        50\n",
      "\n",
      "avg / total       0.97      0.97      0.96       420\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "params = {'C':[0.1, 1, 10, 100],\n",
    "          'gamma':[1e-3, 1e-2, 1e-1, 1e1]}\n",
    "gsc = GridSearchCV(SVC(), params, cv=5)\n",
    "gsc.fit(features_train, labels_train)\n",
    "\n",
    "print('Best Parameters: ', gsc.best_params_)\n",
    "print('SVM Accuracy: ', gsc.score(features_test, labels_test), '\\n')\n",
    "print(classification_report(labels_test, gsc.predict(features_test),\n",
    "                            target_names = target_names))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**PCA and Adaboost**_\n",
    "\n",
    "I thought maybe an ensemble method would model the data better since.. more is typically better.. or in this case, maybe more overfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaboost Accuracy:  1.0 \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    non-POI       1.00      1.00      1.00       370\n",
      "        POI       1.00      1.00      1.00        50\n",
      "\n",
      "avg / total       1.00      1.00      1.00       420\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "adaboost = AdaBoostClassifier(base_estimator = dt)\n",
    "pca = PCA(n_components=5)\n",
    "adc = Pipeline(steps = [('pca',pca), ('adaboost', adaboost)])\n",
    "\n",
    "adc.fit(features_train, labels_train)\n",
    "\n",
    "print('Adaboost Accuracy: ', adc.score(features_test, labels_test), '\\n')\n",
    "print(classification_report(labels_test, adc.predict(features_test),\n",
    "                            target_names = target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation and Evaluation\n",
    "\n",
    "_**Evaluation**_\n",
    "\n",
    "In this project, we are interested in recall and precision, and secondarily, the f1 score, which for our purposes is the averaged recall and precision scores.\n",
    "\n",
    "- recall: true positive/(true positive + false negative), or probability that relevant items are classified\n",
    "- precision: true positive/(true positive + false positive), or probability that items classified are relevant\n",
    "- f1 score: (recall + precision)/2\n",
    "\n",
    "We are probably more focused on recall, as it reflects the probability of identifying POIs even at the risk of misidentifying 'non-interesting' persons (that's what you get for hanging with this crowd!), while precision reflects the probability that those identified are truly of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Validation**_\n",
    "\n",
    "Validation is testing the validity of a model's predictions, usually on a test data set.  A classic mistake is using the entire dataset for both training and testing, which would result in a model that could make seemingly very good predictions but which are actually a result of overfitting.  Thus, separating data into testing and training sets is essential, but the model could still overfit on whatever was apportioned as the test set, thus necessitating a validation test for initial evaluation and a test set for final evaluation.\n",
    "\n",
    "Cross-validation supposedly alleviates this problem, and as mentioned above I decided to use stratified splits for cross-validation.  \n",
    "\n",
    "Using the tester provided by the assignment, boring Naive Bayes fared the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB(priors=None)\n",
      "\tAccuracy: 0.85464\tPrecision: 0.48876\tRecall: 0.38050\tF1: 0.42789\tF2: 0.39814\n",
      "\tTotal predictions: 14000\tTrue positives:  761\tFalse positives:  796\tFalse negatives: 1239\tTrue negatives: 11204\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_classifier(nbc, my_dataset, features_list)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
